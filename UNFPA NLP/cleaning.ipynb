{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#importing statements, libraries we use\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib as mpl\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#building the corpus\n",
    "'''\n",
    "What we do is get the folder downloaded, then read in each file's text\n",
    "From here, we use re to strip any numeric and turn everything into lower case\n",
    "For the sake of this data, that is all the stripping we need to do\n",
    "We save everything to a pandas df, and save that to the disc\n",
    "\n",
    "Output is three pickled files: corpus of raw, cleaned df, document term matrix\n",
    "'''\n",
    "\n",
    "def cleaning(text):\n",
    "    if type(text) != str:\n",
    "        return\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "path = '/mnt/c/Users/Zackw/Downloads/2019 MCP Cohort 2'\n",
    "path2 = '/mnt/c/Users/Zackw/Downloads/Learning Needs 2021 - Survey for Team Coaches'\n",
    "dire = os.listdir(path)\n",
    "files = list(filter(lambda y: y.endswith('.docx'), dire))\n",
    "df = pd.DataFrame(columns=['Text'],\n",
    "                index = files)\n",
    "\n",
    "for file in files:\n",
    "    info = docx2txt.process(path + '/' + file)\n",
    "    df.loc[file] = info\n",
    "\n",
    "df.to_pickle(path + '/' + 'corpus.pkl')\n",
    "\n",
    "#cleaning the text\n",
    "clean = lambda x: cleaning(x)\n",
    "data_clean = pd.DataFrame(df.Text.apply(clean))\n",
    "data_clean.to_pickle(path + '/' + 'data_clean.pkl')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('\\n'\n",
      " \"What we do is get the folder downloaded, then read in each file's text\\n\"\n",
      " 'From here, we use re to strip any numeric and turn everything into lower '\n",
      " 'case\\n'\n",
      " 'For the sake of this data, that is all the stripping we need to do\\n'\n",
      " 'We save everything to a pandas df, and save that to the disc\\n'\n",
      " '\\n'\n",
      " 'Output is three pickled files: corpus of raw, cleaned df, document term '\n",
      " 'matrix\\n')"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#document term matrix\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.Text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm.to_pickle((path + '/' + 'dtm.pkl'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can you add an additional regular expression to the clean_text_round2 function to further clean the text?\n",
    "Play around with CountVectorizer's parameters. What is ngram_range? What is min_df and max_df?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path2 = '/mnt/c/Users/Zackw/Downloads/Learning Needs 2021 - Survey for Team Coaches'\n",
    "df = pd.from_csvDataFrame(columns=['Text'],\n",
    "                index = files)\n",
    "\n",
    "for file in files:\n",
    "    info = docx2txt.process(path + '/' + file)\n",
    "    df.loc[file] = info\n",
    "\n",
    "df.to_pickle(path + '/' + 'corpus.pkl')\n",
    "\n",
    "#cleaning the text\n",
    "clean = lambda x: cleaning(x)\n",
    "data_clean = pd.DataFrame(df.Text.apply(clean))\n",
    "data_clean.to_pickle(path + '/' + 'data_clean.pkl')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "pygments_lexer": "xonsh",
   "codemirror_mode": "shell",
   "mimetype": "text/x-sh",
   "file_extension": ".xsh"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}